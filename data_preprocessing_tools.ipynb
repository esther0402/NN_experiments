{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/esther0402/NN_experiments/blob/main/data_preprocessing_tools.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37puETfgRzzg"
      },
      "source": [
        "# Data Preprocessing Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoRP98MpR-qj"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np #allows to work with arrays (np is the shortcut, which makes it faster)\n",
        "import matplotlib.pyplot as plt #allows us to specifically plot charts\n",
        "import pandas as pd #imports datasets, create matrix of features, vectors, etc"
      ],
      "metadata": {
        "id": "hqHaml16rU05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RopL7tUZSQkT"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('Data.csv')\n",
        "x = dataset.iloc[:,:-1].values\n",
        "y = dataset.iloc[:,-1].values"
      ],
      "metadata": {
        "id": "gKSOq52vsdJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)"
      ],
      "metadata": {
        "id": "eUDZxiMlzAEf",
        "outputId": "ca5dc1b3-ece6-47ff-aaf2-9bc362d13ca4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 nan]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' nan 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "id": "A7tE63rgz7sE",
        "outputId": "5a9679a0-3a94-406c-f6bb-3862082d4b91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nqHSAqOKz7eu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhfKXNxlSabC"
      },
      "source": [
        "## Taking care of missing data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
        "\n",
        "imputer.fit(x[:,1:3])\n",
        "x[:,1:3] = imputer.transform(x[:,1:3])"
      ],
      "metadata": {
        "id": "037rHHc_055M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)"
      ],
      "metadata": {
        "id": "rvD2GQPL5QyA",
        "outputId": "78ff4bbf-998b-4639-c958-b36b8ef81814",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 63777.77777777778]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' 38.77777777777778 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CriG6VzVSjcK"
      },
      "source": [
        "## Encoding categorical data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhSpdQWeSsFh"
      },
      "source": [
        "### Encoding the Independent Variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "#We use 2 classes in sklearn : column transformer class + one hot encoder class\n",
        "\n",
        "ct = ColumnTransformer(transformers = [('encoder', OneHotEncoder(), [0])], remainder = 'passthrough')\n",
        "#We create an object of the column transformer class named ct\n",
        "  #1st arg: transformers -> what transform? what kind of encoding? On which index?\n",
        "  #2nd arg: remainder -> which columns will remain the same?\n",
        "    #passthrough = we keep anything not transformed\n",
        "x = np.array(ct.fit_transform(x))\n",
        "#we fit and transform the result, then transform it to array form"
      ],
      "metadata": {
        "id": "eh6XnHun_sLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7QspewyeBfx",
        "outputId": "50dbb2f9-3d68-4d36-ecdd-5695d1367753",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [0.0 1.0 0.0 30.0 54000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 35.0 58000.0]\n",
            " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXh8oVSITIc6"
      },
      "source": [
        "### Encoding the Dependent Variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "#LaberEncoder will encode the dependent variable into 0s and 1s\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "#This doesn't have to be a NumPy array because it's the dependent variable vector (it's not expected)"
      ],
      "metadata": {
        "id": "kEa-dAqhDbCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyhY8-gPpFCa",
        "outputId": "e7cb9c42-cdc8-4fb7-c08b-d1dad575d82e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 0 1 1 0 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb_vcgm3qZKW"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_text_split\n",
        "#We use the skitlearn library -> contains 'module_selection' module -> contains 'train_test_split' function\n",
        "\n",
        "#these are the outputs\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state = 1)\n",
        "#input of train_test_split function ->\n",
        "  #matrix of features\n",
        "  #dependent variable\n",
        "  #split size (80% training recommended, so 0.2 for test size)\n",
        "    #This means for us, 8 customers will fo in training, and 2 in test set\n",
        "  #random_state (optional?)"
      ],
      "metadata": {
        "id": "lZGkymuCoprV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpGqbS4TqkIR"
      },
      "source": [
        "## Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "#We access the sklearn library to access the preprocessing module, which contains the Standard Scaler class\n",
        "\n",
        "sc = StandardScaler()\n",
        "#It's created as an instance in the StandardScaler class, so we add ()\n",
        "\n",
        "X_train[:,3:] = sc.fit_transform(X_train[:,3:])\n",
        "#We fit our scaler (standardization tool) on the training set\n",
        "#We don't scale our dummy variables (categorical data), so we input the other two remaining columns (age and salaries, from 3rd column)\n",
        "#fit method will get each feature of x_train, compute the mean and standard deviation of the features (age and salary)\n",
        "#transform method will apply the standardization formula\n",
        "\n",
        "X_test[:,3:] = sc.transform(X_test[:,3:])\n",
        "#We just do the transform method because fit will get the mean and sd again, which we don't need"
      ],
      "metadata": {
        "id": "rvF-FxG3os1Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}